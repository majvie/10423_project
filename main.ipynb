{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biasing\n",
    "\n",
    "We have three types of bias: \n",
    "- \"filter\": Ranking bias: The star rating is used to rank the reviews.\n",
    "- \"ranking\": In the context we provide to the LLM, the star rating is used to rank the reviews.\n",
    "- \"prompt\": The LLM is biased by including an explicit instruction in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "TITLE = \"title.y\"\n",
    "\n",
    "# the meta csv joins meta data with the reviews\n",
    "df = pd.read_csv('electronics_reviews_with_meta.csv')\n",
    "df = df[df['training'] == 1]\n",
    "\n",
    "product_code = \"B00004ZCJJ\"\n",
    "matching_reviews = df[df['parent_asin'] == product_code]\n",
    "# display(matching_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biasing import AI_Summarizer\n",
    "from pprint import pprint\n",
    "\n",
    "query = \"image quality\"\n",
    "bias_types = {\n",
    "    \"none\": [],\n",
    "    \"filter\": [],\n",
    "    \"ranking\": [], \n",
    "    \"prompt\": [],\n",
    "}\n",
    "\n",
    "\n",
    "if __name__ ==  \"__main__\":\n",
    "    model = AI_Summarizer(matching_reviews)\n",
    "\n",
    "    for bias_type, bias_objs in bias_types.items():\n",
    "        print(f\"Bias Type: {bias_type}\")\n",
    "        answer_obj = model.get_summary(query, bias_type=bias_type)\n",
    "        bias_objs.append(answer_obj)\n",
    "\n",
    "    # Display the results\n",
    "    for bias_type, answer_obj in bias_types.items():\n",
    "        answer_obj = answer_obj[0]\n",
    "        print(f\"Bias Type: {answer_obj['bias_type']}\")\n",
    "        print(f\"Query: {answer_obj['query']}\")\n",
    "        pprint(f\"Answer: {answer_obj['answer']}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "There are two methods for evaluating the presence of bias:\n",
    " - Sentiment Analysis via BERT: Use a BERT classifier to predict the average star rating of a product given the RAG output.\n",
    "   - Part a: Predict ratings using biased RAG output. Then, compare against the true average rating and compute Mean-Squared Error (MSE).\n",
    "   - Part b: Additionally predict ratings using unbiased RAG output. Then, compare prediction agreement for biased vs. unbiased using COHEN'S KAPPA\n",
    " - Cosine similarity on embeddings: Compute the cosine similarity between biased and unbiased RAG outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import Evaluator\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluator = Evaluator(path_to_RAG_outputs=\"./bias_results.csv\",\n",
    "                          bias_types=[\"filter\", \"ranking\", \"prompt\"])\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
